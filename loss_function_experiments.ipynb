{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04953182-e503-456d-9be3-e5e2b89d67f0",
   "metadata": {},
   "source": [
    "We use echo-evolution data with a two-qubit noise level $q_{2} = 0.01$ and 5 points in time from 0 to $\\pi$. The data are deployed in time in 12000 pairs (ideal-noise). The data is split into training dataset (8000), validation dataset (2000) and test dataset (2000). For different sizes of the hidden layer of the neural network \n",
    "(D_hidden = 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 9, 10, 12, 25, 50, 100, 200) 50 models with different initial parameters are generated. The models are trained for 100 epochs with a batched size of 80 (100 batches total) and a learning rate of $lr = 3*10^{-4}$. The MSE error function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12462ffc-1c9b-4847-b7b6-f6e08cbda64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from '/home/VNIIA/dvbabukhin/Загрузки/paper_code/functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import importlib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "import nn_functions\n",
    "importlib.reload(nn_functions)\n",
    "import functions as func\n",
    "importlib.reload(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a1791-2804-4351-97f2-fd2fc839b605",
   "metadata": {},
   "source": [
    "### Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181101af-42d4-46d4-b9e1-60a11e946cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "new_dir = cwd + '/data_q_01_J_half_h'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d85ed0-32fc-451b-b3ff-63165c3477d7",
   "metadata": {},
   "source": [
    "#### Echo-evolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d31f649-5775-4622-971e-e69fbb867ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 5, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data['data'].shape = (number of initial states, number of time points, noise-free(0) or noisy(1), number of spins)\n",
    "\"\"\"\n",
    "\n",
    "data_name = '/data_t_5'\n",
    "\n",
    "with open(new_dir + data_name + '.pkl', 'rb') as f:\n",
    "    train_data_loaded = pickle.load(f)\n",
    "    \n",
    "del train_data_loaded['parameters']['init state']\n",
    "print(train_data_loaded['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f320ca-651b-4b67-a652-1769a99c8dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (2400, 5, 2, 6)\n",
      "(12000, 6) torch.Size([8000, 6]) torch.Size([2000, 6]) torch.Size([2000, 6])\n",
      "(12000, 6) torch.Size([8000, 6]) torch.Size([2000, 6]) torch.Size([2000, 6])\n",
      "Noisy data vector t:     tensor([ 0.3624, -0.2286, -0.1950, -0.0500, -0.2198, -0.0932],\n",
      "       dtype=torch.float64)\n",
      "Noise-free data vector t:  tensor([ 0.9134, -0.5824, -0.9238, -0.3568, -0.9490, -0.4934],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Convert qubit excitation to spin magnetization\n",
    "X = train_data_loaded['data'][:,:,1,:]\n",
    "X = 2*X - 1\n",
    "y = train_data_loaded['data'][:,:,0,:]\n",
    "y = 2*y - 1\n",
    "\n",
    "# Upload time points\n",
    "time_points = np.linspace(0, train_data_loaded['parameters']['total sim time'], train_data_loaded['parameters']['time points'])\n",
    "tn = X.shape[1] # number of time points\n",
    "\n",
    "# Expand data from different data points\n",
    "X_new = []\n",
    "for k in range(X.shape[1]):\n",
    "    X_new += [*X[:,k,:]]\n",
    "X_new = np.array(X_new)\n",
    "X = X_new\n",
    "\n",
    "y_new = []\n",
    "for k in range(y.shape[1]):\n",
    "    y_new += [*y[:,k,:]]\n",
    "y_new = np.array(y_new)\n",
    "y = y_new\n",
    "\n",
    "# Divide data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1666)\n",
    "X_train_val = torch.from_numpy(X_train_val)\n",
    "X_test  = torch.from_numpy(X_test)\n",
    "y_train_val = torch.from_numpy(y_train_val)\n",
    "y_test  = torch.from_numpy(y_test)\n",
    "\n",
    "# Divide training data into training and validation sets\n",
    "valid_size = 0.2\n",
    "num_train_val = len(X_train_val)\n",
    "indices = list(range(num_train_val))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train_val))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "X_train = X_train_val[train_idx]    \n",
    "y_train = y_train_val[train_idx]\n",
    "\n",
    "X_valid = X_train_val[valid_idx]    \n",
    "y_valid = y_train_val[valid_idx]\n",
    "    \n",
    "print(\"data shape: \", train_data_loaded['data'].shape)\n",
    "print(X.shape, X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(y.shape, y_train.shape, y_valid.shape, y_test.shape)\n",
    "\n",
    "k = 0\n",
    "\n",
    "print(\"Noisy data vector t:    \", X_train[k])\n",
    "print(\"Noise-free data vector t: \", y_train[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f9968b-6ca9-4700-8d85-a630c5f12349",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "new_dir = cwd + '/data_q_01_J_half_h'\n",
    "new_dir += '/loss_experiment'\n",
    "\n",
    "if not os.path.isdir(new_dir):\n",
    "    os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6fec5dd-939c-49c9-9d7e-e00d78acca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'train_data'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "train_data = {\"X\": X_train, \"Y\": y_train}\n",
    "pickle.dump(train_data, output)\n",
    "output.close()\n",
    "\n",
    "file_name = 'valid_data'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "valid_data = {\"X\": X_valid, \"Y\": y_valid}\n",
    "pickle.dump(valid_data, output)\n",
    "output.close()\n",
    "\n",
    "file_name = 'test_data'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "test_data = {\"X\": X_test, \"Y\": y_test}\n",
    "pickle.dump(test_data, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df1c3e-7044-44b2-99cb-36d526f60241",
   "metadata": {},
   "source": [
    "## Training neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c8b0584-754e-469f-8baa-4773b0d141f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "D_in = X_train[0].shape[0]\n",
    "D_hidden_lst = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 25, 50, 100, 200]\n",
    "Num_models = 50\n",
    "epochs = 100\n",
    "\n",
    "lr_0 = 3e-4\n",
    "criterion = nn.MSELoss()\n",
    "N_batches = 80\n",
    "batch_size = int(X_train.shape[0]/N_batches)\n",
    "print(batch_size)\n",
    "\n",
    "model_matrix = [ [nn_functions.generate_onelayer_model(D_in, D_hidden, out_f='tanh') for _ in range(Num_models)] for D_hidden in D_hidden_lst]\n",
    "for models in model_matrix:\n",
    "    for model in models:\n",
    "        model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b3a7b3-8437-4a19-bd4b-3ab8ea780f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dhidden:  1\n",
      "Dhidden:  2\n",
      "Dhidden:  3\n",
      "Dhidden:  4\n",
      "Dhidden:  5\n",
      "Dhidden:  6\n",
      "Dhidden:  7\n",
      "Dhidden:  8\n",
      "Dhidden:  9\n",
      "Dhidden:  10\n",
      "Dhidden:  12\n",
      "Dhidden:  25\n",
      "Dhidden:  50\n",
      "Dhidden:  100\n",
      "Dhidden:  200\n"
     ]
    }
   ],
   "source": [
    "train_losses_array = np.zeros((len(D_hidden_lst), Num_models, epochs))\n",
    "valid_losses_array = np.zeros((len(D_hidden_lst), Num_models, epochs))\n",
    "\n",
    "for k, d in enumerate(D_hidden_lst):\n",
    "    \n",
    "    print(\"Dhidden: \", d)\n",
    "    \n",
    "    models = model_matrix[k]\n",
    "    optimizers = [optim.Adam(model.parameters(), lr=lr_0) for model in models]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Shuffle training set\n",
    "        num_train = len(X_train)\n",
    "        indices = list(range(num_train))\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        # Shuffle validation set\n",
    "        num_valid = len(X_valid)\n",
    "        indices = list(range(num_valid))\n",
    "        np.random.shuffle(indices)\n",
    "        X_valid = X_valid[indices]\n",
    "        y_valid = y_valid[indices]\n",
    "\n",
    "        train_losses = np.zeros(len(models))\n",
    "            \n",
    "        for i in range(N_batches):\n",
    "            \n",
    "            x, y = X_train[i*batch_size:(i+1)*batch_size, :], y_train[i*batch_size:(i+1)*batch_size, :]\n",
    "            \n",
    "            for l, (optimizer, model) in enumerate(zip(optimizers, models)):\n",
    "                optimizer.zero_grad()\n",
    "                y_ = model(x)\n",
    "                loss = criterion(y, y_)\n",
    "                train_losses[l] += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()    \n",
    "        \n",
    "        else:\n",
    "            valid_losses = np.zeros(len(models))\n",
    "            with torch.no_grad():\n",
    "                for x, y in zip(X_valid, y_valid):\n",
    "                    \n",
    "                    for l, model in enumerate(models):\n",
    "                        y_ = model(x)\n",
    "                        loss = criterion(y, y_)\n",
    "                        valid_losses[l] += loss.item()\n",
    "        \n",
    "        for l, _ in enumerate(models):\n",
    "            train_losses_array[k][l][epoch] = train_losses[l]/len(X_train)\n",
    "            valid_losses_array[k][l][epoch] = valid_losses[l]/len(X_valid) \n",
    "            \n",
    "            \n",
    "file_name = 'trained_model_matrix'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "pickle.dump(model_matrix, output)\n",
    "output.close()\n",
    "\n",
    "file_name = 'train_losses'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "pickle.dump(train_losses_array, output)\n",
    "output.close()\n",
    "\n",
    "file_name = 'valid_losses'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "pickle.dump(valid_losses_array, output)\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
