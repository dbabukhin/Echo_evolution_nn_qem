{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814f56c8-9ec3-45d6-9e66-aec103093c4f",
   "metadata": {},
   "source": [
    "This notebook contains data generation procedures, which are further used to train a neural network quantum error mitigation.\n",
    "In particular, here we generate noisy and noise-free vectors of spin magnetizations for echo evolution and forward-in-time evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "437daf60-a5e5-4959-b177-d121eea9cb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from '/home/VNIIA/dvbabukhin/Загрузки/paper_code/functions.py'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from datetime import date\n",
    "import functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64acbbd-66bb-460c-9b10-fb49fcc55e5f",
   "metadata": {},
   "source": [
    "### Echo-evolution data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c2a7d59b-dd5c-4e4a-b0c3-09a94056fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no existing data loaded\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(functions)\n",
    "\n",
    "\"\"\"\n",
    "data['data'].shape = (number of initial states, number of time points, noise-free(0) or noisy(1), number of spins)\n",
    "\"\"\"\n",
    "\n",
    "h = 1.0\n",
    "J = h/2\n",
    "sample_size = 2400\n",
    "\n",
    "noise_dict = {\n",
    "    \"depolarizing\": [0.011],\n",
    "}\n",
    "noise_model = functions.generate_noise_model(noise_dict)\n",
    "num_qubits = 6\n",
    "num_trotters = 10\n",
    "J_array = [J, J, J, J, J, J, J]\n",
    "h_array = [h, h, h, h, h, h]\n",
    "time = np.pi\n",
    "time_points = 5\n",
    "p_th = 0.8\n",
    "coupling_map=[[0,1], [0,2], [1,3], [3,2], [2,4], [3,5], [4,5]]\n",
    "\n",
    "data = functions.generate_data_TFI_echo_dynamics(\n",
    "    sample_size=sample_size,\n",
    "    num_qubits=num_qubits, \n",
    "    num_trotters=num_trotters,\n",
    "    time=time,\n",
    "    time_points=time_points, # should be 2 or more time points\n",
    "    h_array=h_array,\n",
    "    J_array=J_array,\n",
    "    coupling_map=coupling_map,\n",
    "    p_th=p_th,\n",
    "    noise_model=noise_model,\n",
    "    noise_dict=noise_dict,\n",
    "    periodic_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a553b280-2fa5-4dd4-a4f2-aa665df6a715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2401, 5, 2, 6)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "27750b51-e5a0-4ef3-9b52-8de3a2aed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(functions)\n",
    "\n",
    "now = date.today().isoformat()\n",
    "cwd = os.getcwd()\n",
    "new_dir = cwd + '/' + now\n",
    "\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "noise_params = [v for v in noise_dict.values()]\n",
    "noise_params_ = []\n",
    "for p in noise_params:\n",
    "    for par in p:\n",
    "        noise_params_.append(par)\n",
    "                \n",
    "file_name = (data[\"parameters\"][\"backend\"].name() + \"_\"\n",
    "             + \"_\".join(noise_dict.keys()) + \"_\" + \"_\".join(map(str, noise_params_)) \n",
    "             + \"_%d_qubits\"%num_qubits + \"_%d_trotters\"%data[\"parameters\"][\"num of trotters\"]\n",
    "            + \"_%d_timesteps\"%data['data'].shape[1])\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "\n",
    "pickle.dump(data, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52b888-7624-4f24-bbdb-7670fbfebe05",
   "metadata": {},
   "source": [
    "### Forward-in-time-evolution data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f5e222a3-51bc-4955-b6ec-c0a3be157bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1.0\n",
    "J = h/2\n",
    "\n",
    "noise_dict = {\n",
    "    \"depolarizing\": [0.012],\n",
    "}\n",
    "noise_model = functions.generate_noise_model(noise_dict)\n",
    "num_qubits = 6\n",
    "coupling_map=[[0,1], [0,2], [1,3], [3,2], [2,4], [3,5], [4,5]]\n",
    "J_array = [J, J, J, J, J, J, J]\n",
    "h_array = [h, h, h, h, h, h]\n",
    "\n",
    "data_forward = functions.generate_data_TFI_forward_dynamics(\n",
    "    sample_size=100,\n",
    "    num_qubits=num_qubits, \n",
    "    num_trotters=10,\n",
    "    exact_multiplier=10,\n",
    "    time=2*np.pi,\n",
    "    time_points=20,\n",
    "    h_array=h_array,\n",
    "    J_array=J_array,\n",
    "    coupling_map=coupling_map,\n",
    "    p_th=0.8,\n",
    "    noise_model=noise_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e7ade856-118b-47a1-b2f0-16096ba6b39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20, 3, 6)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_forward['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cc3ae5f8-cefc-4415-93c7-a9ff71aeeaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "now = date.today().isoformat()\n",
    "cwd = os.getcwd()\n",
    "new_dir = cwd + '/' + now\n",
    "\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "noise_params = [v for v in noise_dict.values()]\n",
    "noise_params_ = []\n",
    "for p in noise_params:\n",
    "    for par in p:\n",
    "        noise_params_.append(par)\n",
    "        \n",
    "file_name = (\"test_\" + data_forward[\"parameters\"][\"backend\"].name() + \"_\"\n",
    "             + \"_\".join(noise_dict.keys()) + \"_\" + \"_\".join(map(str, noise_params_)) \n",
    "             + \"_%d_qubits\"%num_qubits + \"_%d_trotters\"%data_forward[\"parameters\"][\"num of trotters\"])\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "\n",
    "pickle.dump(data_forward, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fe325073-996e-4fea-ba8d-ede2f1a37d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20, 3, 6)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data\n",
    "\n",
    "cwd = os.getcwd()\n",
    "new_dir = cwd + '/data_q_012_J_half_h'\n",
    "data_name = '/test_qasm_simulator_depolarizing_0.012_6_qubits_10_trotters'\n",
    "#data_name = '/qasm_simulator_depolarizing_0.012_6_qubits_10_trotters_5_timesteps'\n",
    "\n",
    "with open(new_dir + data_name + '.pkl', 'rb') as f:\n",
    "    test_data_loaded = pickle.load(f)\n",
    "    \n",
    "test_data_loaded['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03173db-104d-4bca-9fb2-d3bab12a39b1",
   "metadata": {},
   "source": [
    "#### misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "875e6d79-a379-416a-aac1-be0d94135cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4801, 5, 2, 6)\n",
      "(2401, 5, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "new_dir = cwd + '/data_q_011_J_half_h'\n",
    "\n",
    "data_name = '/train_qasm_simulator'\n",
    "\n",
    "# with open(new_dir + data_name + '.npy', 'rb') as f:\n",
    "#     train_data_loaded = pickle.load(f)\n",
    "    \n",
    "train_data_loaded = np.load(new_dir + '/' + data_name + '.npy')\n",
    "print(train_data_loaded.shape)\n",
    "\n",
    "ind = [i if np.sum(v) != 0 else None for i, v in enumerate(train_data_loaded)]\n",
    "while None in ind:\n",
    "    ind.remove(None)\n",
    "    \n",
    "train_data_loaded = train_data_loaded[ind]\n",
    "print(train_data_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1ed360f7-79cc-440a-9a18-46d566c637c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no existing data loaded\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "h = 1.0\n",
    "J = h/2\n",
    "sample_size = 1\n",
    "\n",
    "noise_dict = {\n",
    "    \"depolarizing\": [0.013],\n",
    "}\n",
    "noise_model = functions.generate_noise_model(noise_dict)\n",
    "num_qubits = 6\n",
    "num_trotters = 10\n",
    "J_array = [J, J, J, J, J, J, J]\n",
    "h_array = [h, h, h, h, h, h]\n",
    "time = np.pi\n",
    "time_points = 5\n",
    "p_th = 0.8\n",
    "coupling_map=[[0,1], [0,2], [1,3], [3,2], [2,4], [3,5], [4,5]]\n",
    "\n",
    "data = functions.generate_data_TFI_echo_dynamics(\n",
    "    sample_size=sample_size,\n",
    "    num_qubits=num_qubits, \n",
    "    num_trotters=num_trotters,\n",
    "    time=time,\n",
    "    time_points=time_points, # should be 2 or more time points\n",
    "    h_array=h_array,\n",
    "    J_array=J_array,\n",
    "    coupling_map=coupling_map,\n",
    "    p_th=p_th,\n",
    "    noise_model=noise_model,\n",
    "    noise_dict=noise_dict,\n",
    "    periodic_save=True)\n",
    "\n",
    "data_dict = {\n",
    "    \"data\": train_data_loaded,\n",
    "    \"parameters\":\n",
    "    {\n",
    "        \"init state\": data['parameters']['init state'],\n",
    "        \"num of qubits\": num_qubits,\n",
    "        \"num of trotters\": num_trotters,\n",
    "        \"total sim time\": time,\n",
    "        \"time points\": time_points,\n",
    "        \"h values\": h_array,\n",
    "        \"J values\": J_array,\n",
    "        \"coupling map\": coupling_map,\n",
    "        \"p threshold\": p_th,\n",
    "        \"circuit\": data['parameters']['circuit'],\n",
    "        \"backend\": data['parameters']['backend']\n",
    "    }\n",
    "}\n",
    "\n",
    "file_name = f'data_t_{time_points}'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "pickle.dump(data_dict, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e90bfc15-97b0-48d9-9938-d8e7142f88b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20, 3, 6)\n",
      "(100, 20, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "new_dir = cwd + '/data_q_013_J_half_h'\n",
    "\n",
    "data_name = '/test_qasm_simulator'\n",
    "\n",
    "# with open(new_dir + data_name + '.pkl', 'rb') as f:\n",
    "#     test_data_loaded = pickle.load(f)\n",
    "\n",
    "test_data_loaded = np.load(new_dir + '/' + data_name + '.npy')\n",
    "\n",
    "print(test_data_loaded.shape)\n",
    "\n",
    "ind = [i if np.sum(v) != 0 else None for i, v in enumerate(test_data_loaded)]\n",
    "while None in ind:\n",
    "    ind.remove(None)\n",
    "    \n",
    "test_data_loaded = test_data_loaded[ind]\n",
    "print(test_data_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "156c0164-4a09-4c27-969f-a59351791792",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1.0\n",
    "J = h/2\n",
    "sample_size = 1\n",
    "\n",
    "noise_dict = {\n",
    "    \"depolarizing\": [0.013],\n",
    "}\n",
    "noise_model = functions.generate_noise_model(noise_dict)\n",
    "num_qubits = 6\n",
    "num_trotters = 10\n",
    "J_array = [J, J, J, J, J, J, J]\n",
    "h_array = [h, h, h, h, h, h]\n",
    "time = np.pi\n",
    "time_points = 20\n",
    "p_th = 0.8\n",
    "coupling_map=[[0,1], [0,2], [1,3], [3,2], [2,4], [3,5], [4,5]]\n",
    "\n",
    "data = functions.generate_data_TFI_forward_dynamics(\n",
    "    sample_size=sample_size,\n",
    "    num_qubits=num_qubits, \n",
    "    num_trotters=num_trotters,\n",
    "    time=time,\n",
    "    time_points=time_points, # should be 2 or more time points\n",
    "    h_array=h_array,\n",
    "    J_array=J_array,\n",
    "    coupling_map=coupling_map,\n",
    "    p_th=p_th,\n",
    "    noise_model=noise_model)\n",
    "\n",
    "data_dict = {\n",
    "    \"data\": test_data_loaded,\n",
    "    \"parameters\":\n",
    "    {\n",
    "        \"init state\": data['parameters']['init state'],\n",
    "        \"num of qubits\": num_qubits,\n",
    "        \"num of trotters\": num_trotters,\n",
    "        \"total sim time\": time,\n",
    "        \"time points\": time_points,\n",
    "        \"h values\": h_array,\n",
    "        \"J values\": J_array,\n",
    "        \"coupling map\": coupling_map,\n",
    "        \"p threshold\": p_th,\n",
    "        \"circuit\": data['parameters']['circuit'],\n",
    "        \"backend\": data['parameters']['backend']\n",
    "    }\n",
    "}\n",
    "\n",
    "file_name = f'data_t_{time_points}'\n",
    "output = open(new_dir + '/{}.pkl'.format(file_name), 'wb')\n",
    "pickle.dump(data_dict, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b289f4-80bd-4be3-b9c6-b5bd6141c18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
